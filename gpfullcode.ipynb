{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10297457,"sourceType":"datasetVersion","datasetId":6373610},{"sourceId":10306856,"sourceType":"datasetVersion","datasetId":6380187},{"sourceId":10329858,"sourceType":"datasetVersion","datasetId":6396098},{"sourceId":10507111,"sourceType":"datasetVersion","datasetId":6504766}],"dockerImageVersionId":30840,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install the BERTopic library for topic modeling and clustering user reviews\n!pip install bertopic","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import the BERTopic model \nfrom bertopic import BERTopic","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the user review dataset into a pandas DataFrame for further processing\nCluster_Data = pd.read_csv('/kaggle/input/mediumfull/dataset (1).csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Pre-processing the dataset","metadata":{}},{"cell_type":"code","source":"# Download the WordNet corpus needed for lemmatization with NLTK\n!python3 -m nltk.downloader wordnet\n!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define a dictionary for expanding common English contractions in text\ncontractions_dict = {\n    \"ain't\": \"am not / are not / is not / has not / have not\",\n    \"aren't\": \"are not\",\n    \"can't\": \"cannot\",\n    \"could've\": \"could have\",\n    \"couldn't\": \"could not\",\n    \"didn't\": \"did not\",\n    \"doesn't\": \"does not\",\n    \"don't\": \"do not\",\n    \"hadn't\": \"had not\",\n    \"hasn't\": \"has not\",\n    \"haven't\": \"have not\",\n    \"he'd\": \"he had / he would\",\n    \"he'll\": \"he shall / he will\",\n    \"he's\": \"he has / he is\",\n    \"I'd\": \"I had / I would\",\n    \"I'll\": \"I shall / I will\",\n    \"I'm\": \"I am\",\n    \"I've\": \"I have\",\n    \"isn't\": \"is not\",\n    \"it's\": \"it has / it is\",\n    \"let's\": \"let us\",\n    \"might've\": \"might have\",\n    \"mightn't\": \"might not\",\n    \"must've\": \"must have\",\n    \"mustn't\": \"must not\",\n    \"needn't\": \"need not\",\n    \"oughtn't\": \"ought not\",\n    \"shan't\": \"shall not\",\n    \"she'd\": \"she had / she would\",\n    \"she'll\": \"she shall / she will\",\n    \"she's\": \"she has / she is\",\n    \"should've\": \"should have\",\n    \"shouldn't\": \"should not\",\n    \"that's\": \"that has / that is\",\n    \"there's\": \"there has / there is\",\n    \"they'd\": \"they had / they would\",\n    \"they'll\": \"they shall / they will\",\n    \"they're\": \"they are\",\n    \"they've\": \"they have\",\n    \"wasn't\": \"was not\",\n    \"we'd\": \"we had / we would\",\n    \"we'll\": \"we shall / we will\",\n    \"we're\": \"we are\",\n    \"we've\": \"we have\",\n    \"weren't\": \"were not\",\n    \"what'll\": \"what shall / what will\",\n    \"what's\": \"what has / what is\",\n    \"when's\": \"when has / when is\",\n    \"where's\": \"where has / where is\",\n    \"who'd\": \"who had / who would\",\n    \"who'll\": \"who shall / who will\",\n    \"who's\": \"who has / who is\",\n    \"won't\": \"will not\",\n    \"would've\": \"would have\",\n    \"wouldn't\": \"would not\",\n    \"you'd\": \"you had / you would\",\n    \"you'll\": \"you shall / you will\",\n    \"you're\": \"you are\",\n    \"you've\": \"you have\"\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Apply contraction expansion to the 'content' column to standardize text for better analysis\ndef expand_contractions(text, contractions_dict):\n    \"\"\"\n    This function replaces contractions in a given text with their expansions\n    based on the provided contractions dictionary.\n    \"\"\"\n    words = text.split()\n    new_words = []\n    for word in words:\n        if word.lower() in contractions_dict:\n            new_words.extend(contractions_dict[word.lower()].split(\"/\"))\n        else:\n            new_words.append(word)\n    return \" \".join(new_words)\n\nCluster_Data[\"content\"] = Cluster_Data[\"content\"].apply(lambda x: expand_contractions(x, contractions_dict))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import necessary libraries for data manipulation, text preprocessing, and lemmatization\nimport pandas as pd\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set up stopwords and lemmatizer\nstop = set(stopwords.words('english'))\nlemma = WordNetLemmatizer()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Drop unnecessary columns\ncolumns_to_drop = ['score', 'appVersion', 'repliedAt', 'replyContent', 'at', 'reviewCreatedVersion', 'thumbsUpCount']\nCluster_Data = Cluster_Data.drop(columns=columns_to_drop, errors='ignore')\nprint(f\"Dropped unnecessary columns. Remaining columns: {list(Cluster_Data.columns)}\")\n\n# Drop rows where 'sentiment' is equal to 'POSITIVE'\ninitial_count = len(Cluster_Data)\nCluster_Data = Cluster_Data[Cluster_Data['sentiment'] != 'POSITIVE']\nremoved_count = initial_count - len(Cluster_Data)\nprint(f\"Removed {removed_count} positive sentiment reviews. Remaining reviews: {len(Cluster_Data)}\")\n\n# Drop rows where the number of words in 'content' is less than or equal to 3\ninitial_count = len(Cluster_Data)\nCluster_Data = Cluster_Data[Cluster_Data['content'].str.split().apply(len) > 3]\nremoved_count = initial_count - len(Cluster_Data)\nprint(f\"Removed {removed_count} short reviews (<= 3 words). Remaining reviews: {len(Cluster_Data)}\")\n\n# Convert reviews to lowercase and split into words\nCluster_Data['NewReviews'] = Cluster_Data['content'].str.lower().str.split()\nprint(\"Converted content to lowercase and split into words.\")\n\n# Remove stopwords\ninitial_count = len(Cluster_Data)\nCluster_Data['NewReviews'] = Cluster_Data['NewReviews'].apply(lambda x: [item for item in x if item not in stop])\nprint(\"Removed stopwords from reviews.\")\n\n# Lemmatize the reviews\nCluster_Data['Cleaned_reviews'] = Cluster_Data['NewReviews'].apply(\n    lambda x: ''.join([lemma.lemmatize(re.sub('[^A-Za-z]', ' ', word)) for word in x]).strip()\n)\nprint(\"Lemmatized the reviews.\")\n\n# Remove duplicate reviews\ninitial_count = len(Cluster_Data)\nCluster_Data = Cluster_Data.drop_duplicates(subset=['Cleaned_reviews'], keep='first')\nremoved_count = initial_count - len(Cluster_Data)\nprint(f\"Removed {removed_count} duplicate reviews. Remaining reviews: {len(Cluster_Data)}\")\n\n# Final DataFrame output\nprint(f\"Final dataset contains {len(Cluster_Data)} reviews.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display the first few rows of the dataset to verify preprocessing steps\nCluster_Data.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prepare and load the cleaned review text data into a list for clustering \nreviews = Cluster_Data['Cleaned_reviews'].dropna().tolist()  # Ensure no NaN values\nprint(f\"Final dataset contains {len(reviews)} reviews.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Clustering the reviews","metadata":{}},{"cell_type":"code","source":"# Initialize and fit BERTopic model on the review data to generate clusters \ntopic_model = BERTopic()\ntopics, probs = topic_model.fit_transform(reviews)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Add the generated topic labels to the original dataset and save to a CSV file\nCluster_Data['Topic'] = topics\nCluster_Data.to_csv('medium_dataset_with_topics.csv', index=False)\nprint(Cluster_Data.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display the topic summary\ntopic_model.get_topic_info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install additional libraries for topic visualization and dimensionality reduction\n!pip install -U plotly pandas umap-learn","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize the topics in 2D space\ntopic_model.visualize_topics()\n\n# Visualize word distributions per topic\ntopic_model.visualize_barchart()\n\n# Visualize the document probabilities\ntopic_model.visualize_documents(reviews)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create and save an HTML visualization of the topic model to review topic distributions\ntopic_model.visualize_topics().write_html(\"topics_visualization.html\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display the frequency of each topic identified by the BERTopic model\ntopic_freq = topic_model.get_topic_freq()\npd.set_option('display.max_rows', None)  # Display all rows\nprint(topic_freq)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import libraries for dimensionality reduction and silhouette score calculation\nimport umap\nfrom sklearn.metrics import silhouette_score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extract the embeddings and topics\nembeddings = topic_model.embedding_model.embedding_model.encode(reviews, show_progress_bar=True)\ncluster_labels = topic_model.get_document_info(reviews)['Topic']\n\n# Apply UMAP to reduce dimensionality for visualization and evaluation\numap_embeddings = umap.UMAP(n_components=3, random_state=42).fit_transform(embeddings)\n\n# Remove outliers (-1 labels)\nvalid_indices = [i for i, label in enumerate(cluster_labels) if label != -1]\nfiltered_embeddings = umap_embeddings[valid_indices]\nfiltered_labels = [cluster_labels[i] for i in valid_indices]\n\n# Calculate the Silhouette Score to assess clustering quality\nsilhouette_avg = silhouette_score(filtered_embeddings, filtered_labels)\nprint(f\"Silhouette Score (UMAP Embeddings): {silhouette_avg}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sort the DataFrame by the 'Topic' column\nsorted_df = Cluster_Data.sort_values(by='Topic', ascending=True)\n\n# Save the sorted dataset to a new CSV file\nsorted_df.to_csv('medium_sorted_reviews_by_topic.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the sorted dataset\nimport pandas as pd\nCluster_Data = pd.read_csv('/kaggle/working/medium_sorted_reviews_by_topic.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Filter out noise points (-1)\nfiltered_data = Cluster_Data[Cluster_Data['Topic'] != -1]\n\n# Save the filtered data\nfiltered_data.to_csv(\"filtered_dataset_without_noise.csv\", index=False)\n\nprint(f\"Number of remaining reviews: {len(filtered_data)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Group by 'Topic' and sample 10% of each group for evaluation\nsampled_data = (\n    filtered_data.groupby('Topic', group_keys=False)\n    .apply(lambda x: x.sample(frac=0.1, random_state=42))  # Adjust random_state for reproducibility if needed\n)\n\n# Save the sampled data to a CSV file\nsampled_data.to_csv(\"sampled_10_percent_per_topic.csv\", index=False)\n\nprint(f\"Number of sampled reviews: {len(sampled_data)}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split the sampled data into 4 equal parts for group evaluation\nimport numpy as np\n\nsplit_data = np.array_split(sampled_data, 4)\n\n# Save each part to a separate CSV file\nfor i, part in enumerate(split_data, start=1):\n    part.to_csv(f\"sampled_data_part_{i}.csv\", index=False)\n    print(f\"Saved part {i} with {len(part)} reviews to 'sampled_data_part_{i}.csv'\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Requirements Extraction","metadata":{}},{"cell_type":"code","source":"# Load the filtered dataset \nimport pandas as pd\nCluster_Data = pd.read_csv(\"/kaggle/input/finalclusters/filtered_dataset_without_noise.csv\")\n\n# Display the first few rows of the dataset\nCluster_Data.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Installing the latest version of the OpenAI library for generating requirements\npip install --upgrade openai","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Combine all content into a single string for each topic\nclustered_reviews = Cluster_Data.groupby('Topic')['content'].apply(lambda x: ' '.join(x)).to_dict()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define a function to generate formal software requirements from grouped review text. It uses a prompt specifying detailed rules for requirement generation, and then add them to the dataset.\nrequirements_list = []\n\ndef generate_formal_requirements(reviews_text, topic_id):\n    prompt = f\"\"\"\n    Generate a list of concise and formal software requirements based on the following reviews (Topic ID: {topic_id}).\nEach requirement should follow these rules:\n1.⁠ ⁠*The subject* should be the software or a specific feature.\n2.⁠ ⁠*The predicate* should describe a condition, action, or result, and must be:\n    - Feasible\n    - Necessary\n    - Unambiguous\n    - Testable\n3.⁠ ⁠Use the following language conventions:\n    - *Shall, **will, and **must* indicate mandatory requirements.\n    - *May* and *should* indicate optional requirements.\n4.⁠ ⁠The requirements should directly address the core functionality, performance, stability, and usability needs mentioned in the reviews.\n5. The requirements should be distinct.\n6.Avoid conflicting requirements.\n7. All the mentioned requirements should be covered.\n\nThe requirements should be clearly written and easy to understand. Avoid including additional details, categories, or prioritization.\n\n*Output Format*:\n1.⁠ ⁠Clearly separate the *Representative Requirement* and *Individual Requirements* sections.\n2.⁠ ⁠The format must be:\n\n    Representative Requirement:\n    - Requirement that represents the cluster.\n\n    Individual Requirements:\n    1. Requirement 1\n    2. Requirement 2\n    3. Requirement 3\n\nEnsure the output strictly follows this format, even if the reviews focus on a single feature or topic. Avoid including additional commentary or rephrasing the structure.\nReviews:\n{reviews_text}\n    \"\"\"\n\n    response = client.chat.completions.create(\n        model=\"gpt-4o\", \n        messages=[\n            {\"role\": \"developer\", \"content\": \"You are a helpful assistant.\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        temperature=0.3  # Reduced for deterministic responses\n    )\n\n    # Extract the content of the first choice's message\n    return response.choices[0].message.content.strip()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Process topics in batches (10 topics per batch) For each batch, generate requirements using the custom function \nclustered_reviews = Cluster_Data.groupby('Topic')['content'].apply(lambda x: \" \".join(x)).to_dict()\nbatch_size = 10\ntopics = sorted(clustered_reviews.keys())\n\nfor i in range(0, len(topics), batch_size):\n    batch_topics = topics[i:i + batch_size]\n    \n    # Create a new DataFrame for the batch\n    batch_data = pd.DataFrame({\n        \"Topic\": batch_topics,\n        \"Content\": [clustered_reviews[topic] for topic in batch_topics],  # All reviews in one cell per cluster\n        \"Generated Requirements\": [\n            generate_formal_requirements(clustered_reviews[topic], topic) for topic in batch_topics\n        ]\n    })\n    \n    # Save the batch to a separate file\n    batch_data.to_csv(f\"clustered_reviews_with_requirements_batch_{i // batch_size + 1}.csv\", index=False)\n\nprint(\"Generated requirements for every 10 clusters and saved each batch to separate files.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Merge these batches to one dataset (final dataset)\nimport os\nimport pandas as pd\n\n# Path to the folder containing batch files\nfolder_path = \"/kaggle/input/batches/batches\"\n\n# Initialize an empty list to store dataframes\ndfs = []\n\n# Loop through all files in the folder\nfor file in os.listdir(folder_path):\n    if file.endswith(\".csv\"):  # Ensure only CSV files are processed\n        file_path = os.path.join(folder_path, file)\n        df = pd.read_csv(file_path)\n        dfs.append(df)\n\n# Concatenate all dataframes into one\nmerged_df = pd.concat(dfs, ignore_index=True)\n\n# Sort the merged dataframe by the 'Topic' column\nmerged_df = merged_df.sort_values(by=\"Topic\", ascending=True)\n\n# Save the sorted dataframe to a new CSV file \nmerged_df.to_csv(\"clusters_with_requirements.csv\", index=False)\n\nprint(\"All batch files have been merged into 'clusters_with_requirements.csv'.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}